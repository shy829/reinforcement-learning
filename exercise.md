#### 第一章

1.左右互搏：

会发生怎样的事情？：与自己互博(有点类似于GAN)，即另一个自己是不断变化的，所以可能学习不到最佳策略，也可能陷入死循环中。

是否会学习到不同的策略？：会学到与固定对手不同的策略。

2.对称性：

如何利用这一点修改学习过程？这种改变怎样改善学习过程？：设置一个与对称性有关的状态，这样可以减少试探搜索空间的大小，训练的时候可以显著提高速度并且减少内存。

假如对方没有利用对称性，我们该利用吗？：不该利用，因为对手是环境的一部分。

是否必然具有相同的价值？：不具有，假如某个角上的位置必然导致失败，就意味着这个信息永远用不着，对称相等的位置也就不一定具有相同的价值了。

3.贪心策略：

会更好还是更差？会出现什么问题？：一般情况下是会更差，因为贪心策略一次性就返回最优策略的概率太低。(DP的背包问题就是一个很好的例子)

4.从试探中学习：

5.其他提升方法：

加大训练次数;设计更好的reward函数(比如在输赢两种奖励里再加一种平局)；采用概率的方式让对手走棋，因为有可能对手会一直走某一步棋，以概率的形式可以更好的探索。



#### 第二章

2.1：

0.5+0.5*0.5=0.75. 0.5的概率贪心动作被选中，另外0.5随机情况中，又有0.5的概率被选中。

2.2：

| Qt(a) | a=1  | a=2  | a=3  | a=4  |
| ----- | ---- | ---- | ---- | ---- |
| t=1   | 0    | 0    | 0    | 0    |
| t=2   | -1   | 0    | 0    | 0    |
| t=3   | -1   | 1    | 0    | 0    |
| t=4   | -1   | -0.5 | 0    | 0    |
| t=5   | -1   | 0.33 | 0    | 0    |

决策做出后根据R计算Q

1/2/3 随机或贪婪

4/5 非贪婪

2.3：

e = 0.01/0.1

0.01时选取最优动作的概率为99.1%，0.1时为91%（t->∞会趋向于最优选择）































